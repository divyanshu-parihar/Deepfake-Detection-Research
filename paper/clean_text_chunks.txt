--- Chunk 1 ---
Detection of deepfakes has become a fundamental issue in digital forensics, but the available detectors consistently fail to perform on the presence of different-than-seen generative models. This drawback, commonly known as the generalization gap, is also caused by the overfitting of models to specific compression signals in its data, rather than the general indicators of manipulation that are independent of the generator. In order to address this fundamental weakness, we propose a class of representation learning models called Artifact-Invariant Representation Learning (AIRL) that is defined to generate high-frequency spectral residuals, which are the typical traces of mathematical kindness generated by generative upsampling and do not change about different synthesis pipelines. We use a two-stream network based on extracting semantic RGB features using EfficientNet-B4 and generating frequency-domain noise using Discrete Cosine Transform (DCT) analysis. Besides, we formulate a supervised contrastive learning and explicitly pool together all the fake faces of a particular generator, regardless of which the underlying generator it relies on, thus inhabitants of manipulation-insensitive feature representations. Extensive cross-dataset evaluation experiments with AUC show our method to achieve 95.4 percentage after training on FaceForensics++ and testing on Celeb-DF, increasing by 30.0 percentage points over the widely used Xception baseline, which becomes 65.4 percent on the task. We also show a high level of resilience to typical post-processing operations, maintaining 89.2% AUC when used in the simultaneous blur and JPEG compression settings, whereas other techniques drop to less than 50. In order to measure the contribution of every element in our system, we release comprehensive ablation experiments and share our codebase with the community to facilitate further improvements in effective, generalizable deepfake detection.
--- Chunk 2 ---
faces morphemed with four neural networks - Deepfakes, Face2Face, FaceSwap, and NeuralTextures. Detectors on test samples that are not used in training (by these same techniques) achieve almost perfect accuracy (often exceeding 99). Nevertheless, such zeal proves to be inappropriate in cross-dataset due diligence. When the same Xception-based detector is used to deal with Celeb-DF a dataset that highlights the best quality deepfakes produced using a wide range of synthesis pipelines, the performance drops to 65.4. This weakness is based on the fundamental delagability between that which is being taught to detectors and that which they must know. Current techniques exploit surface-level associations: the JPEG blocking artifacts, the unique blending border patterns or the presence of resolution differences depending on a selected dataset. All these properties are lost in case of the improvements in the quality of generation or when alternative compression is applied. There are economic and social implications of this failure of generalization. The social media servers which processes billions of pictures daily cannot rely on detectors trained new generative technique by retraining. The police departments investigating potential deepfake crimes need technologies that can still be attached to the task as the attackers have more sophisticated approaches to synthesis. The current trend where one is training larger models of training on more diverse datasets defines a competition that is unsustainable. Our Hypothesis We would propose that the key to successful cross-domain detection lies in focusing on the property that is invariant of the generation process rather than the different attribute of specific implementations. Regardless of the architecture, whether using GAN, diffusion model or autoencoder, all neural image generators struggle with a common computational constraint in that they cannot perform upsampling on a reduced latent environment to produce an image at its full image resolution. This upsampling algorithm, typically implemented as transposed convolutions or nearest-neighbor interpolation followed by convolution, forms discrete patterns in the frequency domain. These patterns manifest themselves as repeated artifacts in spectral components with high frequencies, the residue of the checkerboard effects of fractionally-strided convolutions. Notably, the artifacts are practical across different generator architectures because they can be as a result of shared primitives of computations rather than implementation-centric decisions.

--- Chunk 3 ---
potential aliasing effects. Although advanced anti-aliasing methods can reduce the apparent artifacts, it is still possible to see the underlying spectral artifacts by careful inspection. This learning makes us emphasise the significance of frequency-domain representations as a technique to achieve generalizable detection. Contributions We summarize the contributions as follows: Identify We describe an Artifact-Invariant Representation Learning protocol which explicitly distinguishes between content-level features and process-level traces, which facilitates their detection when the generators represent different types of generators. We present a dual-stream architecture which fuses an RGB stream and frequency stream which separates high-frequency DCT residuals, where upsampling artifacts tend to occur. Our training goal involves a directed contrastive learning and this compiles all synthetic faces together irrespective of the generator providing the face, compelling the network to understand characteristics that do not change as a result of the changes. We perform extensive cross-dataset and cross-degradation experiments where we display a high cross-domain generalization result of 95.4. We present ablation studies in a comprehensive way that evaluates the impact of all of the architectural parts and design decisions, as well as failure cases and visualization studies. The structure of this paper is presented in the following way: Section II discusses relevant literature on the deepfake detection and emphasizes the weaknesses of the existing methods. Section III gives an elaborate description of our methodology. Section IV is a presentation of the setup of the experiment. Section V is a valuable perspective and analysis. The 6th section covers other research, such as the analysis of failures and the illustrations. Section VII is about and restrictions and possible avenues. Section VIII concludes. Related Literature Early Detection Approaches The first attempts to detect deepfakes focused on the features of soft synthesis that were easy to detect. Matern et al. researched the physiological indicators, as early deepfakes had abnormal blinking of the eyes and unnatural head movement. Their implementation yielded encouraging fresh portrayal on first-time deepfakes, yet efficacy diminished as synthesis systems embraced aspects of time modeling. Face distortion artifacts as a consequence of resolution were used by Li and Lyu.




--- Chunk 4 ---
pose estimation, theorizing that exchanged faces would show pose discrepancies with the original video setting. Although successful against initial face-swapping techniques, this strategy was shown to be susceptible to reenactment methods such as Face2Face that preserve stable 3D structure. These initial methods found some success but had a shared drawback: they focused on particular, amendable artifacts. As generative techniques advanced to fix noted flaws, detection effectiveness declined accordingly. Deep Learning Methods The shift to deep learning resulted in considerable improvements on benchmark datasets. Afchar et al. introduced MesoNet, a streamlined CNN architecture tailored for extracting mesoscopic features in the detection of fake faces. Their main observation was that manipulation artifacts appear at a mid-level scale — too detailed for semantic evaluation but too broad for pixel-level examination. Although MesoNet is computationally efficient, its restricted capacity hinders its effectiveness in detecting subtle manipulation signs, especially in high-quality deepfakes. Rössler et al. created FaceForensics++ as a standard benchmark and showed that transfer learning from ImageNet with Xception reached 99.2. Zhou et al. proposed a two-stream network that analyzes face and steganalysis features, inspired by the insight that manipulation frequently produces detectable traces via noise analysis. Their strategy enhanced efficiency in certain types of manipulation but faced challenges with techniques that maintain noise properties. Nguyen et al. introduced capsule networks for detecting deepfakes, utilizing capsules' capacity to represent part-whole relationships in facial configurations. Despite demonstrating potential on controlled benchmarks, capsule networks turned out to be costly in terms of computation and highly sensitive to hyperparameter selections. Frequency-Domain Approaches Understanding that features in the spatial domain frequently carry biases specific to the dataset led to an exploration of frequency-domain representations. Durall et al. initially noted that images produced by GANs display distinct spectral artifacts, especially in high-frequency areas where upsampling creates regular patterns. Their investigation showed that even the most advanced GANs do not replicate the complete spectral distribution of natural images, resulting in noticeable artifacts. F3-Net integrated local frequency data with spatial characteristics using

--- Chunk 5 ---
a multi-fork structure. They proposed a frequency-aware decomposition module that captures local frequency representations prior to merging with spatial pathways. Although showing better generalization, their early fusion approach caused the network to depend mainly on spatial features, restricting the use of frequency information. Qian et al. expanded on frequency-aware detection by presenting adaptive spectral feature extraction that adjusts to various types of manipulation. The Frequency in Face Forgery Network (F3-Net) attained top-tier results on multiple benchmarks, yet still showed considerable deterioration when faced with domain shift. Liu et al. introduced SPSL (Spatial-Phase Shallow Learning), which captures phase spectra in addition to amplitude data. Their investigation showed that phase components contain complementary forgery signatures, especially in face-swapping techniques where amplitude spectra stay mostly unchanged. The shallow network design highlighted the significance of maintaining frequency information across the network instead of permitting deep layers to obscure spectral details. Luo et al. recently concentrated on high-frequency elements via gradient-based feature extraction, resulting in enhanced performance across different datasets. Their analysis validated that high-frequency areas hold the most transferable forgery signatures, but their gradient-focused method demands costly computations and faces challenges with highly compressed images where gradient signals diminish. Attention mechanisms have been investigated as a way to concentrate detector capabilities on distinguishing areas. Zhao et al. developed multi-scale attention networks that flexibly prioritize facial areas according to their likelihood of manipulation, enabling the network to focus on boundary regions and other susceptible areas. Dang et al. fused attention with manipulation segmentation, simultaneously forecasting binary authenticity labels and pixel-level manipulation masks. This multi-task approach prompted the network to concentrate on areas specific to manipulation, although it demanded mask annotations that are not consistently present in detection datasets. Vision Transformers (ViT) have lately been utilized for deepfake detection with varied outcomes. Although self-attention facilitates the modeling of long-range dependencies that may help identify global inconsistencies, conventional ViT


--- Chunk 6 ---
architectures require significantly more training data than CNNs to achieve comparable performance. Coccomini et al. demonstrated that hybrid CNN-Transformer architectures can capture both local texture details and global structural coherence, achieving competitive results on benchmark datasets though at substantial computational cost. Wodajo and Atnafu explored efficient attention mechanisms for real-time detection, proposing lightweight self-attention modules that balance computational cost with detection accuracy. Their work highlighted the trade-offs between model capacity and deployment practicality in real-world scenarios. Contrastive and Self-Supervised Learning Contrastive learning has emerged as a powerful paradigm for learning transferable representations. Chen et al.'s SimCLR framework demonstrated that contrastive pre-training on unlabeled data yields features that transfer effectively to downstream tasks. The key insight was that learning to distinguish between augmented views of the same image encourages representations that capture semantic content while ignoring superficial variations. Khosla et al. extended this framework to the supervised setting, showing that class-aware positive pair selection further improves representation quality. Their supervised contrastive loss explicitly encourages samples from the same class to cluster together in embedding space while pushing apart samples from different classes. In deepfake detection, Chen et al. applied contrastive learning to learn manipulation-agnostic representations, treating all synthetic faces as a single positive class regardless of generation method. Their approach improved cross-dataset generalization but relied exclusively on RGB features, missing frequency-domain information critical for detecting subtle artifacts. Zhao et al. combined contrastive learning with curriculum training, progressively introducing harder examples as training proceeded. This approach improved robustness to challenging cases but required careful hyperparameter tuning to balance curriculum difficulty. Limitations of Prior Work Despite significant progress, existing methods share common limitations: enumerate Dataset overfitting : Most approaches achieve high in-domain accuracy but degrade substantially on unseen datasets, indicating memorization of dataset-specific artifacts rather than learning of fundamental forgery signatures. Sensitivity to

--- Chunk 7 ---
compression : Many detectors fail when test images undergo different compression than training data, suggesting reliance on compression artifacts rather than synthesis traces. Incomplete frequency utilization : While frequency-domain methods show promise, most either fuse frequency and spatial features too early (allowing spatial dominance) or focus narrowly on specific frequency bands. Generator-specific features : Few approaches explicitly encourage learning features that generalize across generator types, instead implicitly hoping that sufficient data diversity will induce invariance. Limited robustness analysis : Most evaluations focus on clean test conditions, with limited analysis of behavior under realistic degradations common in social media pipelines. enumerate Our work addresses these limitations through architectural choices that enforce strict separation of content and trace information, combined with explicit training objectives that reward generator-agnostic representations and comprehensive evaluation under varied conditions. Methodology Overview Our Artifact-Invariant Representation Learning framework processes input face images through two parallel streams that extract complementary information: enumerate RGB Stream : A semantic feature extractor based on EfficientNet-B4, pretrained on ImageNet, captures high-level facial attributes and visible manipulation artifacts such as unnatural expressions, inconsistent lighting, or boundary discontinuities. Frequency Stream : A dedicated pathway that converts input images to the frequency domain via DCT, applies high-pass filtering to isolate artifact-containing regions, and processes the resulting feature maps through a lightweight CNN. enumerate Features from both streams are concatenated and passed through fusion layers before classification. Crucially, we train with a combined objective that includes both cross-entropy classification loss and supervised contrastive loss, the latter explicitly encouraging the network to cluster all synthetic faces together regardless of their generation method. Fig. illustrates the complete architecture. Frequency Stream Design The frequency stream is designed to isolate high-frequency spectral residuals where upsampling artifacts concentrate. We employ the Discrete Cosine Transform (DCT), chosen over alternatives like Fourier transform for its

--- Chunk 8 ---
superior energy compaction properties and natural alignment with block-based image processing commonly used in image compression standards. Grayscale Conversion Given an input RGB image , we first convert to grayscale since color information primarily encodes content rather than synthesis traces: These coefficients match the ITU-R BT.601 standard, ensuring consistent grayscale conversion across different image sources. While color channels potentially contain forgery information (e.g., color bleeding at boundaries), our experiments indicated that luminance alone captures sufficient artifact information while reducing computational cost. Block-wise DCT Computation The grayscale image is partitioned into non-overlapping blocks, matching the standard JPEG block size. This alignment is intentional: many deepfake artifacts interact with JPEG compression boundaries, and processing at the same block granularity allows our network to learn these interactions. For each block, we compute the 2D DCT: where when , and otherwise. The DCT coefficients represent frequency components, with being the DC component (average intensity) and higher indices corresponding to higher spatial frequencies. The 2D DCT is separable, allowing efficient computation as sequential 1D transforms along rows then columns. We implement this using precomputed basis matrices for GPU-accelerated processing, achieving negligible overhead compared to forward pass through the CNN backbone. High-Pass Filtering Low-frequency DCT coefficients encode global intensity variations and dominant structural features — information about "what" is depicted rather than "how" it was generated. Semantic content concentrates in these low-frequency components, while synthesis artifacts manifest in high-frequency regions. We apply a high-pass filter that zeros the top-left quadrant of each DCT block: equation This filtering retains only high-frequency components where upsampling artifacts manifest as periodic patterns. The 44 cutoff was selected empirically; ablation studies indicated that smaller cutoffs (retaining more frequencies) introduced content leakage, while larger cutoffs (aggressive filtering) removed some artifact information. Fig. visualizes this filtering process. Frequency Feature Extraction The filtered DCT

--- Chunk 9 ---
Blocks are reorganized into a single-channel feature map that aligns with the spatial dimensions of the input. This feature map undergoes processing by a lightweight CNN that consists of three convolutional blocks, each including: convolution with a stride of 2 for downsampling spatially, Batch normalization for regularization and stability during training, and ReLU activation for introducing non-linearity. Channel dimensions evolve as . This step-by-step growth enables the network to acquire more abstract frequency representations while ensuring computational efficiency. Global average pooling compresses the final feature map into a 256-dimensional vector, which is fed through a fully connected layer with dropout () to generate the ultimate frequency stream representation. We set up convolutional weights with Kaiming initialization and train all parameters of the frequency stream from the ground up since pretrained weights from ImageNet cannot be directly applied to frequency-domain inputs. RGB Stream Design The RGB stream utilizes EfficientNet-B4 pretrained on ImageNet as a primary feature extractor. EfficientNet's compound scaling approach harmonizes network depth, width, and input resolution to attain impressive performance at a reasonable computational expense. The B4 variant offers a beneficial balance between capacity and efficiency for our objective. ImageNet pretraining offers strong low-level feature extraction (edges, textures, patterns) that transfers well to facial analysis. Although faces represent a distinct area, the general visual characteristics acquired during pretraining enhance convergence speed and boost final performance relative to training from the beginning. We eliminate the initial classification head and obtain features following global average pooling, resulting in a 1792-dimensional representation. In the training process, we utilize a differential learning rate approach: layers in the backbone (pretrained on ImageNet) get a learning rate reduced by a factor of 0.1 compared to the newly-initialized layers (fusion, classification head). This method avoids severe loss of pretrained features while enabling adaptation of upper layers for specific tasks. Feature Fusion Architecture combines RGB and frequency features to create a unified representation: Late fusion (merging after separate processing) guarantees that no stream is compromised


--- Chunk 10 ---
prevails in the initial training phase. In contrast, early fusion methods enable the network to prioritize simpler spatial features over the more challenging frequency stream. The combined representation flows through fusion layers: itemize Linear projection: Batch normalization ReLU activation Dropout () itemize The final 512-dimensional fused representation supports both classification and contrastive learning goals. The standard cross-entropy training for binary classification (real vs. fake) does not specifically promote the acquisition of generator-agnostic features. A network might reach low training loss by recalling unique generator signatures instead of identifying shared manipulation patterns. To tackle this, we propose a supervised contrastive learning objective inspired by the framework of Khosla et al. . The main insight is that by explicitly mandating all fake samples to be alike in the embedding space — irrespective of their generator source — we prompt the network to uncover features that generalize across different manipulation techniques. We map the combined representation to a reduced-dimensional embedding space: where the MLP consists of two linear layers with ReLU activation (512, 256, 128) and the final output is normalized to reside on the unit hypersphere. Lowering dimensions and normalization are essential: they stop the embedding space from collapsing and ensure that cosine similarity serves as a significant distance metric. The supervised contrastive loss considers all fake samples as positive pairs, irrespective of their generator source: where represents the collection of all samples in the batch, indicates the set of positive samples for a given sample (same class), signifies cosine similarity, and is a temperature parameter that regulates the concentration of the distribution. This goal specifically incentivizes representations that group all fake faces together while keeping them distinct from real faces, as illustrated in Fig. . Combined Training Objective Our ultimate training goal merges cross-entropy classification loss with contrastive loss: where it balances both components. Classification forecasts originate from


--- Chunk 11 ---
a linear layer implemented on the combined representation: The cross-entropy loss guarantees distinctive predictions while the contrastive loss structures the representation space for better generalization. Ablation studies verified that both loss components impact final performance, with the contrastive term especially crucial for generalization across datasets. Training Procedure We utilize this training procedure: list Initialization: EfficientNet-B4 backbone initialized using ImageNet weights; all additional layers initialized with Kaiming initialization. Optimization: Adam optimizer with an initial learning rate, weight decay, and beta parameters. Learning rate strategy: Cosine annealing with warm restarts (period of 10 epochs), lowest learning rate. Regularization: Dropout() applied in the fusion and frequency stream layers; gradient clipping (maximum norm 1.0) to ensure stability. Data enhancement: Horizontal flipping, random brightness/contrast ($ 20 Early stopping: Training stops if validation AUC fails to improve for 10 straight epochs. enumerate Training generally converges within 30-40 epochs, taking about 8 hours on one NVIDIA A100 GPU. Experimental Setup Datasets We assess multiple datasets that cover various generation techniques and quality tiers: FaceForensics++ (FF++) : The primary benchmark consisting of 1000 authentic YouTube videos altered by four techniques: Deepfakes (autoencoder-driven face swapping), Face2Face (facial reenactment), FaceSwap (graphics-driven face swapping), and NeuralTextures (neural texture rendering). Every technique generates 1000 altered videos. We utilize the c23 (light compression) variant based on previous research, while also assessing c40 (heavy compression) for robustness evaluation. Celeb-DF (v2): A demanding dataset that consists of 590 authentic videos of 59 celebrities alongside 5639 generated videos produced using an advanced deepfake algorithm. The synthesis pipeline resolves numerous artifacts found in previous techniques, leading to improved visual quality and reduced visible signs of manipulation. The substantial domain shift from FF++ establishes this as the main benchmark for cross-dataset evaluation. DFDC Preview: A portion of the Deepfake Detection Challenge dataset curated by Facebook, showcasing varied

--- Chunk 12 ---
topics, contexts, and techniques of manipulation. Includes 1131 genuine and 4113 counterfeit videos featuring diverse ethnicities, lighting scenarios, and compression rates. Employed for further validation across datasets. DeeperForensics-1.0: An extensive dataset created for comprehensive assessment, featuring 60,000 videos with seven types of distortions, such as compression, blur, noise, variations in color saturation, and local block-wise alterations. Employed specifically for assessing robustness. Data preprocessing for face extraction adheres to a standardized pipeline to guarantee uniform assessment: perform face detection using MTCNN with a confidence threshold of 0.95, executed per-frame for video datasets. Alignment: Landmark-based alignment utilizing five facial keypoints (centers of the eyes, tip of the nose, corners of the mouth) for standard frontal pose. Cropping: Faces are cropped with 30. Normalization: Pixel values are adjusted to [0, 1] and subsequently normalized utilizing ImageNet statistics (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]). enumerate For training, we select 10 frames evenly from each video to ensure balanced per-video representation while maintaining a manageable dataset size. To evaluate, we utilize all gathered frames and present video-level predictions derived from averaging the scores at the frame level. Training Augmentations Data augmentation enhances sample diversity while preventing artifacts that could disrupt frequency-domain analysis: itemize Horizontal flipping (50 Random brightness adjustment ($ 20 Random contrast adjustment ($ 20 Gaussian noise (, 20 JPEG compression (quality 70-100, 30 itemize Importantly, we incorporate JPEG compression augmentation to boost resilience against different compression levels often found in real-world situations. The range for compression quality (70-100) is cautious to prevent the loss of authentic forgery artifacts during training. Evaluation Protocol We adhere to the standard cross-dataset evaluation method: Train only on the FF++ (c23) training subset Validate on the FF++ validation subset for hyperparameter tuning Test on the reserved FF++ test subset (in-domain assessment) Test on Celeb-DF, DFDC, and DeeperForensics without fine-tuning (cross-dataset evaluation) This approach assesses true generalization: models are unable to adjust to the characteristics of the test distribution. Metrics for Evaluation We present the ensuing metrics:


--- Chunk 13 ---
itemize AUC : Area Under the ROC Curve, offering a performance metric that is independent of thresholds. This is our main metric since it reflects the discriminative capacity at all operating points. Accuracy: Classification precision at ideal threshold (optimizing Youden's J statistic on validation dataset). Delivers understandable performance at a particular operating point. EER: Equal Error Rate, the point at which the rates of false positives and false negatives are the same. Beneficial for security applications where equal error rates are preferred. List For video-level assessment, we compute the average of frame-level prediction scores and set thresholds on the computed average score. Implementation Details: Framework: PyTorch 2.0, Hardware: NVIDIA A100 GPU (40GB), Batch size: 32 (effective, with gradient accumulation if necessary), Training epochs: 50 maximum, typically early-stopped around 35, Training time: Approximately 8 hours on FF++, Inference speed: 63 FPS on A100, 28 FPS on RTX 3080. Baseline Methods: We evaluate the following methods: MesoNet: Compact network for mesoscopic features, Xception: Standard ImageNet-pretrained baseline, EfficientNet-B4: Our RGB stream backbone without frequency stream, F3-Net: Frequency-aware multi-branch network, SPSL: Spatial-phase shallow learning, Face X-ray: Blending boundary detection. All baseline methods are trained with their designated hyperparameters on FF++ using the identical preprocessing pipeline for equitable comparison. The Results In-Domain Evaluation Table shows findings on the FF++ test set. All approaches demonstrate excellent performance in this context, indicating that in-domain deepfake detection is significantly resolved. The central table shows our approach equals the top baseline (Xception) on in-domain assessment. This outcome verifies that our architectural changes do not compromise in-domain performance while offering advantages in generalization. The Cross-Dataset Generalization Table showcases important cross-dataset findings, in which models trained on FF++ are assessed on Celeb-DF without any modifications


--- Chunk 14 ---
DeeperForensics. center table Steady enhancements are noted across all assessment datasets, affirming that our method generalizes widely instead of overfitting to particular dataset traits. The Ablation Studies Table measures the impact of every structural element. center table Key findings: itemize The frequency stream alone achieves 72.8 Late fusion reaches 78.4 Contrastive learning contributes an additional 6.3 itemize The effect of DCT Filter Cutoff Table investigates the influence of the high-pass filter cutoff. center table A cutoff at 4 offers the best balance: lower cutoffs permit semantic content leakage, which hinders generalization; higher cutoffs eliminate artifact information. The Contrastive Loss Analysis Table examines the impact of the weight of contrastive loss. The center table shows the ideal balance. Increased weights diminish in-domain performance as the contrastive objective starts to take precedence over classification. The Per-Manipulation Analysis Table dissects performance based on manipulation categories. The central table shows consistent performance across these manipulation types. The minor enhancement in NeuralTextures (+0.4) to Robustness to Image Degradations Table assesses robustness in managed degradations. central table When combining blur and JPEG compression, Xception deteriorates by 20.1 This resilience arises from the frequency stream's emphasis on statistical features of high-frequency patterns rather than specific coefficient values. Though degradation reduces high-frequency energy, the proportion of artifacts continues to be distinguishable. The Computational Analysis Table contrasts computational demands. center table Our approach introduces a small overhead (3.5ms, 29 Additional Analysis Visualization of Acquired Features To comprehend what our model grasps, we examine attention patterns and embedding spaces. RGB Stream Attention: Applying gradient-weighted class activation mapping (Grad-CAM) to the RGB stream shows attention on facial edges, eye sections, and mouth regions—areas where noticeable manipulation artifacts usually emerge. Intense activations at blend edges suggest the network is capable of recognizing apparent seams typical in face-swapping techniques. Frequency Stream Patterns: Illustrating the strong activation of certain DCT coefficients shows a preference for diagonal high-frequency elements, aligning with theoretical expectations regarding transposed data.


--- Chunk 15 ---
convolution distortions. The identified patterns are uniform across various types of manipulations, reinforcing our theory that frequency artifacts are applicable across different generators. The t-SNE visualization of the learned embeddings in the embedding space analysis shows the impact of contrastive learning: In the absence of contrastive loss, embeddings mainly group according to manipulation type. Deepfakes, Face2Face, FaceSwap, and NeuralTextures create identifiable clusters, while authentic faces occupy a different area. This organization shows that the network acquires features specific to the generator. Using contrastive loss: All fake samples combine into one dense cluster distinctly separated from genuine faces. The lack of generator-specific structure verifies that contrastive learning effectively promotes manipulation-agnostic representations. Analysis of Failure Cases We examine instances where our approach does not succeed in recognizing failure modes: High-quality diffusion results: Recent diffusion-driven generators (Stable Diffusion, Midjourney) generate faces with no noticeable upsampling artifacts. Our approach attains 74.2 with highly compressed inputs: At maximum compression (JPEG Q=20), our method drops to 68.3. Limited face alterations: When just a small area of the face is altered (e.g., eye substitution), our comprehensive analysis might overlook specific artifacts. Adding attention guided by segmentation may resolve this issue. Temporal Consistency Although our approach treats frames individually, we examine if the predictions show temporal consistency: In video sequences, the predictions at the frame level are strongly correlated (average Pearson correlation 0.87), suggesting that our features reflect stable manipulation traits instead of fleeting noise. This uniformity allows dependable video-level scoring via straightforward frame averaging. Discussion of Why Frequency Analysis is Effective The effectiveness of our frequency-oriented method arises from fundamental characteristics of neural image creation. Every generator encounters the same computational difficulty: increasing the resolution from a compressed latent code to a complete image. This upsampling, whether through transposed convolution, nearest-neighbor interpolation, or learned upsampling, creates distinctive patterns in high-frequency spectral elements. Transposed convolutions, specifically, create checkerboard artifacts due to uneven overlaps in fractionally-strided calculations. Diligent architectural design (for instance, bilinear upsampling succeeded by


--- Chunk 16 ---
Convolution can minimize these artifacts visually, yet their spectral signatures remain detectable by learned analysis. Our DCT-based method separates these signatures by specifically removing low-frequency elements that convey semantic meaning. This division stops the network from using shortcuts linked to facial identity or expression, compelling it to depend on generation-process traces. Limitations and Future Work Although our results are robust, our method has constraints that indicate areas for future investigation: Diffusion models: The iterative denoising mechanism of diffusion models produces distinct spectral signatures compared to explicit upsampling. Broadening our strategy to include diffusion-specific artifacts — potentially via the examination of denoising patterns or variance features — signifies a crucial pathway. Temporal modeling: Handling frames separately overlooks temporal discrepancies (flickering, unnatural movement, inconsistent identity) that could offer extra detection cues. Utilizing 3D convolutions or temporal transformers could enhance detection at the video level. Adversarial robustness: We do not assess resilience against adversarial modifications specifically crafted to avoid detection. Adversarial attacks might aim at either stream, especially the frequency stream's vulnerability to certain spectral patterns. Interpretability: Although we offer visualization analysis, the formal understanding of what frequency patterns signify manipulation is still constrained. Creating understandable frequency forensics could support human analysts and enhance confidence in automated choices. Deployment Considerations Practical implementation necessitates tackling various factors: Threshold selection: Various applications need distinct operating points. Social media moderation might prioritize high recall to identify the majority of fakes, tolerating some false positives. Forensic analysis might prioritize accuracy to prevent erroneous allegations. Our AUC-centered assessment offers adaptability across operational levels. Ensemble methods: No individual detector offers complete coverage. Utilizing our technique together with complementary methods (boundary detection, biological signal analysis, metadata forensics) in an ensemble enhances detection reliability. Ongoing adjustment: With advancements in generation technology, detection models need to be refreshed. Creating systems for ongoing data gathering and model updating is


--- Chunk 17 ---
crucial for sustained efficiency. Conclusion We introduced a framework for Artifact-Invariant Representation Learning aimed at generalizable deepfake detection, which attains leading performance across different domains. Our main insight is that concentrating on invariant features of the generation process — particularly high-frequency spectral residuals — facilitates detection that adapts across generator types. Our dual-stream design integrates EfficientNet-B4 for semantic RGB characteristics alongside a DCT-based frequency approach that separates upsampling artifacts. Supervised contrastive learning actively promotes generator-independent representations by grouping all synthetic faces together without regard to their source. Collectively, these elements produce 95.4. As deepfake technology progresses, detection should likewise adapt. The foundations of our methodology — the division of content and process, clear invariance goals, and multi-stream integration — establish a basis for advanced detectors that can adapt to upcoming synthesis techniques. We provide our implementation to facilitate ongoing investigation in this important field. Acknowledgment We appreciate the open-source community for the tools and datasets that enabled this research. Grateful appreciation to the developers of PyTorch, FaceForensics++, and Celeb-DF for their vital infrastructure and benchmark resources. We also recognize the valuable conversations with peers in the digital forensics field


