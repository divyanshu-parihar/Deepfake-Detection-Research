# -*- coding: utf-8 -*-
"""Pegasus_Humanizer_Colab.ipynb

Automatically generated by Gemini CLI.

Original Author: Divyanshu Parihar
adapted for Google Colab usage.

How to use:
1. Open https://colab.research.google.com/
2. Create a new Notebook.
3. Copy each section below into a code cell and run them in order.
4. Ensure you set "Runtime" -> "Change runtime type" -> "T4 GPU" for speed.
"""

# ==========================================
# CELL 1: INSTALL DEPENDENCIES
# ==========================================
# !pip install transformers sentencepiece sentence-transformers nltk scipy numpy torch

import torch
import nltk
import logging
import numpy as np
from typing import List

# Download NLTK data
nltk.download('punkt')

# Configure Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("Humanizer")

print("Dependencies installed and imported.")

# ==========================================
# CELL 2: DEFINE ENGINES (Burstiness + Pegasus)
# ==========================================

class BurstinessEngine:
    """
    Selects the best paraphrase candidate based on:
    1. Semantic Similarity (must mean the same thing).
    2. Structural Distance (must look different).
    3. Rhythm Contrast (Short sentences follow long ones).
    """
    def __init__(self, device):
        from sentence_transformers import SentenceTransformer
        # sentence-transformers is efficient, runs fine on CPU or GPU
        self.model = SentenceTransformer('all-MiniLM-L6-v2', device=device)
        
    def select_best_candidate(self, original: str, candidates: List[str], prev_length: int) -> str:
        if not candidates:
            return original

        # 1. Semantic Similarity
        embeddings = self.model.encode([original] + candidates)
        original_emb = embeddings[0]
        candidate_embs = embeddings[1:]
        
        from scipy.spatial.distance import cosine
        sim_scores = [1 - cosine(original_emb, cand) for cand in candidate_embs]

        # 2. Length Analysis (Rhythm)
        target_len_type = "short" if prev_length > 20 else "long" if prev_length < 10 else "any"
        
        scored_candidates = []
        for i, cand in enumerate(candidates):
            sim = sim_scores[i]
            
            if sim < 0.85: # strict semantic check
                continue
                
            words = cand.split()
            curr_len = len(words)
            
            score = sim * 1.0
            
            # Boost for contrast
            if target_len_type == "short" and curr_len < 15:
                score += 0.15
            elif target_len_type == "long" and curr_len > 15:
                score += 0.15
            
            # Boost for vocabulary variation (Jaccard)
            orig_set = set(original.lower().split())
            cand_set = set(cand.lower().split())
            if not orig_set.union(cand_set):
                 jaccard = 1.0
            else:
                 jaccard = len(orig_set.intersection(cand_set)) / len(orig_set.union(cand_set))
            
            score += (1 - jaccard) * 0.2
            
            scored_candidates.append((score, cand))
        
        if not scored_candidates:
            return candidates[0]
            
        scored_candidates.sort(key=lambda x: x[0], reverse=True)
        return scored_candidates[0][1]

class PegasusHumanizer:
    def __init__(self):
        from transformers import PegasusForConditionalGeneration, PegasusTokenizer
        
        # Use GPU if available
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Loading Pegasus on {self.device}...")
        
        model_name = "tuner007/pegasus_paraphrase"
        self.tokenizer = PegasusTokenizer.from_pretrained(model_name)
        self.model = PegasusForConditionalGeneration.from_pretrained(model_name).to(self.device)
        
        self.burst_engine = BurstinessEngine(self.device)
        logger.info("Models loaded successfully.")

    def humanize_sentence(self, sentence: str, prev_length: int) -> str:
        if len(sentence.split()) < 4:
            return sentence

        text = sentence
        # Prepare input
        batch = self.tokenizer([text], padding='longest', max_length=60, truncation=True, return_tensors="pt").to(self.device)
        
        # Generate with high variability
        translated = self.model.generate(
            **batch,
            max_length=60,
            num_beams=10, 
            num_return_sequences=5, 
            temperature=1.5,
            do_sample=True,
            top_k=50,
            top_p=0.95
        )
        
        candidates = self.tokenizer.batch_decode(translated, skip_special_tokens=True)
        
        best = self.burst_engine.select_best_candidate(sentence, candidates, prev_length)
        return best

    def humanize_paragraph(self, text: str) -> str:
        sentences = nltk.sent_tokenize(text)
        new_sentences = []
        prev_len = 0
        
        for i, sent in enumerate(sentences):
            # Simple progress log
            if i % 5 == 0:
                print(f"Processing sentence {i+1}/{len(sentences)}...")
            new_sent = self.humanize_sentence(sent, prev_len)
            new_sentences.append(new_sent)
            prev_len = len(new_sent.split())
            
        return " ".join(new_sentences)

# Initialize Model (Run this once)
humanizer = PegasusHumanizer()

# ==========================================
# CELL 3: RUN ON YOUR TEXT
# ==========================================

# Replace this text with your content
input_text = """

The quick brown fox jumps over the lazy dog. 
Deep learning has revolutionized the field of artificial intelligence.
"""

print("\nOriginal Text:\n", input_text)
print("-" * 40)

result = humanizer.humanize_paragraph(input_text)

print("\nHumanized Text:\n", result)
print("-" * 40)

# ==========================================
# OPTIONAL CELL: UPLOAD FILE
# ==========================================
# from google.colab import files
# uploaded = files.upload()

# for filename in uploaded.keys():
#     print(f"Processing {filename}...")
#     with open(filename, 'r') as f:
#         content = f.read()
#     output = humanizer.humanize_paragraph(content)
#     with open(f"humanized_{filename}", "w") as f:
#         f.write(output)
#     files.download(f"humanized_{filename}")
